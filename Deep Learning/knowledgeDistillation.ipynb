{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets,transforms\n",
    "from tqdm import tqdm\n",
    "from d2l import torch as d2l\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先训练老师模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义老师模型的网络\n",
    "class TeacherNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TeacherNet,self).__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Conv2d(1,32,3,1),nn.ReLU(),\n",
    "            nn.Conv2d(32,64,3,1),nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.3),nn.Flatten(),\n",
    "            nn.Linear(9216,128),nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(128,10),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(model,device,train_loader,optimizer,epoch):\n",
    "    model.train()\n",
    "    trained_samples=0\n",
    "    train_loss,train_acc=[],[]\n",
    "    \n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target=data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        loss=F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples+=len(data)\n",
    "        progress=math.ceil(batch_idx/len(train_loader)*50)\n",
    "        print(\"\\rTrain epoch %d:%d/%d,[%-51s]%d%%\" % (epoch,trained_samples,len(train_loader.dataset),'-'*progress+'>',progress*2),end='')\n",
    "        # acc=(output.argmax(dim=-1)==target).float().mean()\n",
    "\n",
    "        # #记录损失和精度\n",
    "        # train_loss.append(loss.item())\n",
    "        # train_acc.append(acc)\n",
    "        # print(f'训练周期：{epoch+1:03d}/{num_epoch:03d} 训练损失：{train_loss:.5f} 训练精度：{train_acc:.5f}')\n",
    "\n",
    "def test_teacher(model,device,test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target =data.to(device),target.to(device)\n",
    "            output=model(data)\n",
    "            test_loss+=F.cross_entropy(output,target,reduction='sum').item()\n",
    "            pred=output.argmax(dim=1,keepdim=True)\n",
    "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss/=len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest:average loss:{:.4f},accuracy:{}/{} ({:.0f}%)'.format(test_loss,correct,len(test_loader.dataset),100.*correct/len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss,correct/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_main():\n",
    "    num_epochs=10\n",
    "    batch_size=64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader=torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"../data/MNIST\",train=True,download=\"True\",\n",
    "                        transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,),(0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size,shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader=torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"../data/MNIST\",train=False,download=True,\n",
    "                        transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,),(0.3081,))\n",
    "        ])),\n",
    "        batch_size=1000,shuffle=True\n",
    "    )\n",
    "\n",
    "    model=TeacherNet().to(device)\n",
    "    optimizer=torch.optim.Adadelta(model.parameters())\n",
    "\n",
    "    teacher_history=[]\n",
    "    max_acc=0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_teacher(model,device,train_loader,optimizer,epoch)\n",
    "        loss,acc=test_teacher(model,device,test_loader)\n",
    "\n",
    "        teacher_history.append((loss,acc))\n",
    "        if acc>max_acc:\n",
    "            torch.save(model.state_dict(),\"teacher.pt\")\n",
    "            max_acc=acc\n",
    "    \n",
    "    return model,teacher_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0510,accuracy:9854/10000 (99%)\n",
      "Train epoch 1:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0406,accuracy:9871/10000 (99%)\n",
      "Train epoch 2:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0323,accuracy:9888/10000 (99%)\n",
      "Train epoch 3:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0352,accuracy:9883/10000 (99%)\n",
      "Train epoch 4:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0332,accuracy:9898/10000 (99%)\n",
      "Train epoch 5:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0313,accuracy:9908/10000 (99%)\n",
      "Train epoch 6:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0337,accuracy:9908/10000 (99%)\n",
      "Train epoch 7:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0313,accuracy:9911/10000 (99%)\n",
      "Train epoch 8:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0298,accuracy:9913/10000 (99%)\n",
      "Train epoch 9:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0315,accuracy:9915/10000 (99%)\n"
     ]
    }
   ],
   "source": [
    "teacher_model,teacher_history=teacher_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#让老师教学生网络\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(StudentNet,self).__init__()\n",
    "        self.fc1=nn.Linear(28*28,128)\n",
    "        self.fc2=nn.Linear(128,64)\n",
    "        self.fc3=nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=torch.flatten(x,1)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        output=F.relu(self.fc3(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#关键：定义KD的loss\n",
    "def distillationLoss(y,labels,teacher_scores,temp,alpha):\n",
    "    return nn.KLDivLoss()(F.log_softmax(y/temp,dim=1),F.softmax(teacher_scores/temp,dim=1))*(temp*temp*2.0*alpha)+F.cross_entropy(y,labels)*(1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher=TeacherNet()\n",
    "teacher.load_state_dict(torch.load(\"teacher.pt\"))\n",
    "\n",
    "def train_student_kd(model,device,train_loader,optimizer,epoch):\n",
    "    model.train()\n",
    "    trained_samples=0\n",
    "\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target=data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        teacher_output=teacher(data)\n",
    "        loss=distillationLoss(output,target,teacher_output,temp=5.0,alpha=0.7)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        trained_samples+=len(data)\n",
    "        progress=math.ceil(batch_idx/len(train_loader)*50)\n",
    "        print(\"\\rTrain epoch %d:%d/%d,[%-51s]%d%%\" % (epoch,trained_samples,len(train_loader.dataset),'-'*progress+'>',progress*2),end='')\n",
    "\n",
    "def test_student_kd(model,device,test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target=data.to(device),target.to(device)\n",
    "            output=model(data)\n",
    "            test_loss+=F.cross_entropy(output,target,reduction=\"sum\").item()\n",
    "            pred=output.argmax(dim=1,keepdim=True)\n",
    "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss/=len(test_loader.dataset)\n",
    "\n",
    "    print(\"\\nTest:average loss:{:.4f},accuracy:{}/{} ({:.0f}%)\".format(test_loss,correct,len(test_loader.dataset),100.*correct/len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss,correct/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_kd_main():\n",
    "    epochs=10\n",
    "    batch_size=64\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader=torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"../data/MNIST\",train=True,download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,),(0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size,shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader=torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"../data/MNIST\",train=False,download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,),(0.3081,))\n",
    "        ])),\n",
    "        batch_size=1000,shuffle=True\n",
    "    )\n",
    "\n",
    "    model=StudentNet().to(device)\n",
    "    optimizer=torch.optim.Adadelta(model.parameters())\n",
    "\n",
    "    student_history=[]\n",
    "    max_acc=0.0\n",
    "    for epoch in range(1,epochs+1):\n",
    "        train_student_kd(model,device,train_loader,optimizer,epoch)\n",
    "        loss,acc=test_student_kd(model,device,test_loader)\n",
    "        student_history.append((loss,acc))\n",
    "\n",
    "        if acc>max_acc:\n",
    "            torch.save(model.state_dict(),\"student_kd.pt\")\n",
    "            max_acc=acc\n",
    "    \n",
    "    return model,student_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weirdo/anaconda3/envs/DeepLearning/lib/python3.8/site-packages/torch/nn/functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.1555,accuracy:9663/10000 (97%)\n",
      "Train epoch 2:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.1268,accuracy:9716/10000 (97%)\n",
      "Train epoch 3:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0989,accuracy:9774/10000 (98%)\n",
      "Train epoch 4:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0888,accuracy:9796/10000 (98%)\n",
      "Train epoch 5:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0795,accuracy:9802/10000 (98%)\n",
      "Train epoch 6:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0785,accuracy:9821/10000 (98%)\n",
      "Train epoch 7:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0689,accuracy:9817/10000 (98%)\n",
      "Train epoch 8:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0823,accuracy:9809/10000 (98%)\n",
      "Train epoch 9:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0798,accuracy:9818/10000 (98%)\n",
      "Train epoch 10:60000/60000,[-------------------------------------------------->]100%\n",
      "Test:average loss:0.0669,accuracy:9816/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "student_kd_model,student_kd_history=student_kd_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student(model,device,train_loader,optimizer,epoch):\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for data,target in tqdm(train_loader):\n",
    "        data,target=data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        loss=F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch:%d finished' % epoch)\n",
    "\n",
    "def test_student(model,device,test_loader):\n",
    "    model.eval()\n",
    "    loss=0\n",
    "    acc=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target=data.to(device),target.to(device)\n",
    "            output=model(data)\n",
    "            loss+=F.cross_entropy(output,target,reduction='sum').item()\n",
    "            pred=output.argmax(dim=1,keepdim=True)\n",
    "            acc+=pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss/=len(test_loader.dataset)\n",
    "    \n",
    "    print(f'test acc:{acc/len(test_loader.dataset)},average loss:{loss}')    \n",
    "\n",
    "    return loss,acc/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_main():\n",
    "    train_loader=torch.utils.data.DataLoader(\n",
    "            datasets.MNIST(\"../data/MNIST\",train=True,download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,),(0.3081,))\n",
    "            ])),\n",
    "            batch_size=64,shuffle=True\n",
    "        )\n",
    "\n",
    "    test_loader=torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\"../data/MNIST\",train=False,download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,),(0.3081,))\n",
    "        ])),\n",
    "        batch_size=1000,shuffle=True\n",
    "    )\n",
    "\n",
    "    epochs=10\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model=StudentNet().to(device)\n",
    "    optimizer=torch.optim.Adadelta(model.parameters())\n",
    "    max_acc=0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_student(model,device,train_loader,optimizer,epoch+1)\n",
    "        loss,acc=test_student(model,device,test_loader)\n",
    "        \n",
    "        if acc>max_acc:\n",
    "            torch.save(model.state_dict(),\"student.pt\")\n",
    "            max_acc=acc\n",
    "        \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 finished\n",
      "test acc:0.8709,average loss:0.3392891387939453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 44.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 finished\n",
      "test acc:0.8779,average loss:0.31562213897705077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 finished\n",
      "test acc:0.8806,average loss:0.30705920104980466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 finished\n",
      "test acc:0.8787,average loss:0.3131327026367188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 finished\n",
      "test acc:0.8856,average loss:0.2978225158691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 finished\n",
      "test acc:0.8816,average loss:0.31645680236816404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 44.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 finished\n",
      "test acc:0.8807,average loss:0.32659322814941405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:22<00:00, 41.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 finished\n",
      "test acc:0.8821,average loss:0.32430076904296873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 44.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 finished\n",
      "test acc:0.8835,average loss:0.3294613433837891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 45.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 finished\n",
      "test acc:0.8836,average loss:0.3229833068847656\n"
     ]
    }
   ],
   "source": [
    "model=student_main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cddc6a761772f09bdc816d4c8a9607d53bc5eb5813e332343549772becad59b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
