# 循环神经网络从0开始实现

[code](../Deep%20Learning/RNN从零开始实现.ipynb)

## 独热编码

对于词表中不同的次元数为n，设一个长度为n的向量，初始化为全0，对应词元就在其对应位置上置0为1。

如果将词元看作一个标量的话，每次采样形状是二维向量：(批量大小，时间步数)。onehot会将其变成三维的，就是将词元不看作标量，而是看作一个张量。之后还会转换采样的维度，将其从（批量大小，时间步数，词表大小）变成（时间步数，批量大小，词表大小）。这样更方便更新小批量数据的隐状态。

## 初始化模型参数

隐藏单元数num_hiddens是一个可以调节的超参数。训练语言模型时，输入和输出来自相同词表。因此他们具有相同的维度。
```py
def get_params(vocab_size,num_hiddens,device):
    num_inputs=num_outputs=vocab_size

    def normal(shape):
        return torch.randn(size=shape,device=device)*0.01

    #隐藏层的参数
    W_xh=normal((num_inputs,num_hiddens))
    W_hh=normal((num_hiddens,num_hiddens))
    b_h=torch.zeros(num_hiddens,device=device)

    #输出层的参数
    W_out=normal((num_hiddens,num_outputs))
    b_out=torch.zeros(num_outputs,device=device)

    #附加梯度
    params=[W_xh,W_hh,W_out,b_h,b_out]
    for param in params :
        param.requires_grad_(True)
    return params
```
## 循环神经网络模型

定义循环神经网络前需要一个init_run_state函数在初始化的时候返回隐状态。

以后可能会遇到隐状态包括多个变量的情况，所以这里返回一个元组。
```py
def init_rnn_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )
```

rnn函数通过输入的最外层维度实现循环，这样能够逐步更新小批量数据的隐状态。
```py
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量大小，词表大小)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # X的形状：(批量大小，词表大小)
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```
定义一个类来包装这些函数，并且存储从0开始实现循环神经网络模型的参数。
```py
class RNNModelScratch: #@save
    """从零开始实现的循环神经网络模型"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
```

## 预测

prefix是用户提供的包含多个字符的字符串，我们需要预测prefix后的新字符。循环遍历prefix的时候，不断将隐状态传递到下一个时间步，但是不输出，这叫做预热期。在此期间，模型会自我更新，但不会进行预测。
```py
def predict_ch8(prefix, num_preds, net, vocab, device):  #@save
    """在prefix后面生成新字符"""
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]:  # 预热期
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # 预测num_preds步
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

## 梯度裁减

计算T个时间步上的梯度会在反向传播的时候产生长度为O(T)的矩阵乘法链。T比较大的时候，可能会导致梯度消失或者梯度爆炸。可以将梯度的值投影回给定半径的球来裁剪梯度。
$$\mathbf{g} \leftarrow \min \left(1, \frac{\theta}{\|\mathbf{g}\|}\right) \mathbf{g}$$
```py
def grad_clipping(net, theta):  #@save
    """裁剪梯度"""
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm
```

## 训练

与之前的训练不同的是，当使用顺序分区的时候，下个batch的第i个子序列与当前batch第i个子序列是相邻的，如果每一个隐状态都可以在同一周期的不同批次之间传播的话，会产生很复杂的计算量。因此我们将隐状态的梯度分离在每个batch内部。