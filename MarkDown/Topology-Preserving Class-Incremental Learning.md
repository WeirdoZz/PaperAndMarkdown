# Topology-Preserving Class-Incremental Learning

## 摘要

为了减轻灾难性遗忘，作者提出了通过维持一个网络特征空间的拓扑结构来保持旧类的知识。TPCIL用弹性赫布图来对特征空间的拓扑结构建模，这一过程是通过竞争性赫布学习规则实现的。为了维持拓扑结构，作者提出了拓扑保持损失来惩罚增量学习情形下的弹性赫布图的邻接关系的改变。

## 引言

将网络用于分类问题时，我们常常先假设数据的类是预定义好的并且是固定的，然后再构造一个有和类别数相等的数量神经元的输出层的神经网络。但是，实际应用中经常会出现之前没有遇到过的类，他们不能被之前学到的模型识别。

类增量学习旨在学习一个统一的分类器能识别新类而不忘记旧类。在训练新类时无法获取旧类的数据集的实际状况下，这一问题需要得到研究。一个直接的方法是在新类上微调模型。但是，这非常容易灾难性遗忘。

为了解决灾难性遗忘，许多类增量学习方法应用了知识蒸馏技术来保持保存在网络输出中的旧类的知识。知识蒸馏技术本来是为了将教师模型的隐知识转移到学生模型而提出的。在将知识蒸馏使用到类增量学习时，模型会保存少量的有代表性的旧类样本图片，并且将蒸馏损失和分类损失以某种方式结合来从新类的训练样本中学习。

尽管知识蒸馏能够在某种程度上减轻灾难性遗忘，他们仍然会遇到新旧类样本不平衡引发的偏置问题。此外，作者在实验中观察到基于蒸馏的方法似乎先要忘记旧知识然后重新从旧类的样本中学习知识，这被称作从头开始现象。如下图所示。这样带来的后果就是，模型需要花费额外的周期来重新获取旧类的知识。此外，过量的重新学习也会增加过拟合旧类样本的风险。这些问题限制了从无限多的新类中进行增量学习的能力

![picture 1](https://i.imgur.com/DFXATew.png)  

作者的主要贡献包括：

- 为有效的不遗忘的类增量学习提出了一个神经科学激发的拓扑保持框架
- 通过竞争性赫布学习构造了一个弹性赫布图来对CNN的特征空间的拓扑结构进行建模
- 设计了拓扑保持损失来维持特征空间的拓扑结构并且减少遗忘

## 相关工作

近来的增量学习研究有两个分支。多任务增量学习旨在学习一系列独立的任务，每个任务都有特定的分类器，而单任务增量学习使用一个统一的分类器来将整个增量学习过程作为一个任务看待。本篇就属于但任务增量学习。

### 多任务增量学习

多任务增量学习是在假设任务标识在训练和测试过程中总是已知的情况下进行的。相关研究工作通常采用以下方法来减轻灾难性遗忘：

1. 正则化策略：在学习新的任务的时候对网络权重施加正则。比如，EWC和SI对网络权重加以限制，惩罚对旧任务比较重要的类的权重的变化。
2. 结构化策略，通过扩展，修剪或者隐藏神经连接来动态调整网络的结构。比如，PackNet通过剪枝为新任务创造自由的参数。HAT学习注意力隐藏在学习新任务的时候来限制旧任务的权重
3. 恢复型策略，学习新任务的时候对网络重现对过去的旧任务的经验的记忆。比如，GEM使用一个外部记忆池来存储一组小的旧任务的样本图片并且使用他们来限制学习新类过程中旧类的损失。DGR和LifelongGAN使用一个生成模型来记忆旧任务的数据分布，这个生成模型带有一个生成对抗网络学习去产生旧类的伪训练样本。

总之，多任务方法通过特定任务的分类器在任务层面进行增量学习。这些方法不能直接被类增量学习拿来使用。

### 类增量学习

一项iCaRL的早期工作解耦了分类器的学习和特征表示，其中分类器是通过情景记忆中预先存储的样本的最近匹配来实现的。当学习新类的表征时，蒸馏损失项会加到交叉熵损失函数上，以此来维持旧类样本的表征。之后的EEIL研究以一个端到端的方式通过交叉蒸馏损失学习网络。它克服了iCaRL的局限性，将学习表征和分类器结合到一起。

更多近来的研究表明存在一个由新旧类不均衡训练样本数量引发的关键的偏置问题，即分类层的权重和输出在增量学习之后会偏向新类。为了解决偏置问题，LUCIR归一化了分类层的特征向量和权重，采用余弦距离误差，并且对特征空间进行蒸馏而不是对logits蒸馏。BIC提出偏置修正技术，它学习一个线性模型来统一输出logits的分布。IL2M提出了多记忆方法来不使用蒸馏损失对模型进行微调。它存储旧类的样本的数据来修正预测得分。

与上述方法不同的，TPCIL通过限制表征点之间的关系来维持特征空间的拓扑结构，同时允许表征的移动来适应新类。

## 拓扑保持类增量学习

### 问题定义

类增量学习问题的定义如下。

X，Y和Z分别表示训练集，标签集和测试集。CNN模型$\theta$要从一系列训练组$X^1$,$X^2$,...,$X^T$,$X^T+1$...增量地学习一个统一的分类器，其中$X^T={(x_i^t,y_i^t)}_{i=1}^{N_t}$是带有$N_t$个样本的第t组被标签好的训练集。，$x_i^t$和 $y_i^t$∈$Y^t$ 是第i个图片和他的标签。$Y^t$是所有组中不相交的标签集，比如,$\forall p \neq q, Y^{p} \cap Y^{q}=\varnothing$。在t+1组中，模型$\theta^{t+1}$从$X^{t+1}$中学习得来，不需要旧类的训练集$X^1$,$X^2$,...,$X^T$。之后$\theta^{t+1}$在所有遇到过的测试集$\bigcup_{j=1}^{t+1}Z^j$上进行评估。

### 总体框架

CNN可以看作是一个带有参数$\theta$特征提取器$f(·;\theta)$和一个带有权重矩阵W的分类层的组合。给定一个输入x，CNN输出$o(x;\theta)=W^Tf(x;\theta)$，之后会跟一个softmax层来获取；不同类的概率。用 $F\subseteq R^n$表示由$f(·;\theta)$定义的特征空间。最开始，在基础类训练集$X^1$上通过交叉熵损失训练$\theta^1$。然后，在$X^2$,...,$X^T$,$X^T+1$...上增量式地微调模型，来获取$\theta^2$,...$\theta^t$,$\theta^{t+1}$,...。在第t+1组时，输出层通过增加$|Y^{t+1}|$个新神经元来适应新类。直接在$X^{t+1}$微调$\theta^{t+1}$会覆盖掉$\theta^t$中对于识别旧类很重要的权重，这会打乱特征空间的拓扑结构并且导致灾难性遗忘，在$\bigcup_{j=1}^{t+1}Z^j$的认知表现会下降。

为了维持旧类的拓扑结构，作者首先用弹性赫布图对特征空间拓扑结构建模，然后提出拓扑保持损失项来惩罚弹性赫布
图表示的特征空间拓扑的变化。$G^t$表示在t组时构建的弹性赫布图。t+1组的总体损失函数定义为：

$$\ell\left(X^{t+1}, G^{t} ; \theta^{t+1}\right)=\ell_{C E}\left(X^{t+1}, G^{t} ; \theta^{t+1}\right)+\lambda \ell_{T P L}\left(G^{t} ; \theta^{t+1}\right) \tag{1}$$

上式中，$\ell_{C E}$是标准交叉熵损失：

$$\ell_{C E}\left(X^{t+1}, G^{t} ; \theta^{t+1}\right)=\sum_{(x, y)}-\log \hat{p}_{y}(x) \tag{2}$$

其中(x,y)表示训练样本和他的标签，$\hat{p}_{y}(x)$是CNN预测的给定输入x的属于标签y的可能性。我们同时使用$X^{t+1}$和分配给弹性赫步图节点的旧类图片进行训练。$\ell_{T P L}$就是作者提出的拓扑保持损失项。超参数$\lambda$用来控制拓扑保持损失对总损失的影响。

![picture 2](https://i.imgur.com/S23pfxz.png)  

上图是拓扑保持的可视化结构。金色曲线表示特征空间流型。圆圈和实线表示弹性赫布图的节点和边。（a）随机挑选N个点来初始化弹性赫布图的节点。（b）通过竞争性赫布学习，特征空间被分为N个不相交的维诺室，每一个室都由一个节点定义。邻接关系他通过节点之间的连接描述。（c）为了新类而微调CNN可能会极大地改变节点的邻接关系并且扰乱特征空间的拓扑结构。（d）拓扑损失项迫使弹性赫布图维持节点之间的关系。（e）在学习到新类之后，弹性赫布图会通过插入新的节点进行扩张。所有的节点通过竞争赫布学习更新，相似性会重新计算。

## 弹性赫布图的拓扑建模

对特征空间拓扑结构进行建模的一个有效方法是在特征空间流型上进行竞争性赫布学习。竞争性赫布学习能够学习代表任何流型的一组节点，并且已经证明能够很好地保存拓扑结构。为类增量学习使用拓扑建模并且和CNN结合，作者设计了用竞争性赫布学习构建的弹性赫布图。细节算法如下。

为了计算的稳定性，作者归一化了特征空间并且应用了余弦相似度度量。用$\overline{·}$表示归一化操作，即$\overline{f}=f/||f||$。给定归一化后的特征空间$\overline{F}$，弹性赫布图被表示为$G=<V,E>$，其中 $V=\left\{\overline{\mathbf{v}}_{1}, \cdots, \overline{\mathbf{v}}_{N} \mid \overline{\mathbf{v}}_{i} \in \overline{\mathcal{F}}\right\}$是表示$\overline{F}$的N个节点，E是描述邻接关系的边集。每一个节点$\overline{v}_i$都是表示一个邻接区域$\mathcal{V}_{i}$的特征向量的质心向量。，被称作维诺室:

$$\mathcal{V}_{i}=\left\{\overline{\mathbf{f}} \in \overline{\mathcal{F}} \mid \overline{\mathbf{f}}^{\top} \overline{\mathbf{v}}_{i} \geq \overline{\mathbf{f}}^{\top} \overline{\mathbf{v}}_{j}, \forall j \neq i\right\}, \forall i \tag{3}$$

为了获取到$\overline{v_i}$，我们首先在特征空间中随机挑选一个位置初始化它的值。然后通过以下规一化赫布规则迭代更新$\overline{v_i}$：

$$\mathbf{v}_{i}^{*}=\overline{\mathbf{v}}_{i}+\epsilon \cdot e^{-k_{i} / \alpha}\left(\overline{\mathbf{f}}-\overline{\mathbf{v}}_{i}\right), \overline{\mathbf{v}}_{i}^{*}=\mathbf{v}_{i}^{*} /\left\|\mathbf{v}_{i}^{*}\right\|, i=1, \cdots, N， \tag{4}$$

其中，$\overline{v}_i^*$表示更新后的节点，$e^{-k_{i}}$是用于缩放更新步的衰退函数，$\overline{f}$是输入的新的特征向量。衰退因子由接近等级$k_i$衡量，$\overline{v}_i$是所有节点中离$\overline{f}$第$k_i$近的节点。超参数$\epsilon$是学习率，$\alpha$控制衰退的强度。公式（4）保证了离$\overline{f}$最近的节点所作的调整是最大的，离得远的节点受到的影响就比较小。一直执行公式(4)直到$\overline{v}_i^*$收敛。

随着点集V的更新，作者可能会构建相关的德劳内图来对节点的邻接关系建模，如图2（b）。但是在剃度下降的框架下很难去直接约束德劳内图，因为邻接节点一直在通过赫布规则变化，这很难和CNN的反向传播相配合。为了易于优化，作者将G转化为相似度图作为替代。每一个边$e_{ij}$被分配一个权重$s_{ij}$，表示$\overline{v}_i$和$\overline{v}_j$间的相似度：

$$
s_{i j}=\overline{\mathbf{v}}_{i}^{\top} \overline{\mathbf{v}}_{j} \tag{5}
$$

通过这种方式，G的变化可以反响传播到CNN并且通过剃度下降进行优化。为了计算每个节点在下一组增量学习中的观测值，作者对节点$\overline{v}_i$赋了一个旧类样本中特征向量离他最近的一个图像$u_i$。

在将弹性赫布图用于增量学习时，作者首先通过基类的训练数据构建图。对$\theta^1$的训练完成时，再提取$X^1$上的归一化特征向量，这样就获得了
$\overline{F}^1={\overline{f}(x;\theta^t)|\forall(x,y)\in X^1}$。$\overline{F}^1$形成了基类的特征空间流型。在$\overline{F}^1$上用公式4和5计算赫布弹性图$G^1$。$G^1$被保存以在下一组减轻遗忘。不断迭代下去，在第t+1组时，在学习到$\theta^{t+1}$之后，作者更新之前保存的弹性赫布图$G^t$来是他和变化的特征空间保持一致。通过插入K个新的节点{$\overline{v}_{N+1}$,...,$\overline{v}_{N+K}$}得到新的节点集$V^{t+1}$，然后在$\overline{F}^{t+1}$用公式4更新所有节点。这之后，会计算相似度得到边集$E^{t+1}$。新形成的新类的流型的拓扑通过新节点建模并且整合到弹性赫布图上。

## 拓扑保持限制

在第t+1组，给定弹性赫布图$G^t=<V^t,E^t>$，灾难性遗忘发生的时候，$G^t$随着旧边的混乱而扭曲。为了减轻灾难性遗忘，$G^t$上的连接应当在CNN在$X^{t+1}$上微调的时候做好维护。这可以通过限制由边的权重（比如相似度）描述的节点邻接关系来实现。为了实现这个目标，一个方法是在训练过程中维持边权重的排名。但是，优化非平滑的总体排序是困难且低效的，但局部排序又不能很好地保持总体关系。

作为替代，作者通过计算边权重的初始值和观测值的相关性来衡量邻接关系的变化。较低的相关性就表明这个边的相关性有很大可能在学习过程中发生了变化，而这种变化应当被惩罚。在此基础上，作者定义了拓扑保持损失项：

$$
\ell_{T P L}\left(G^{t} ; \theta^{t+1}\right)=-\frac{\sum_{i, j}^{N}\left(s_{i j}-\frac{1}{N^{2}} \sum_{i, j}^{N} s_{i j}\right)\left(\tilde{s}_{i j}-\frac{1}{N^{2}} \sum_{i, j}^{N} \tilde{s}_{i j}\right)}{\sqrt{\sum_{i, j}^{N}\left(s_{i j}-\frac{1}{N^{2}} \sum_{i, j}^{N} s_{i j}\right)^{2}} \sqrt{\sum_{i, j}^{N}\left(\tilde{s}_{i j}-\frac{1}{N^{2}} \sum_{i, j}^{N} \tilde{s}_{i j}\right)^{2}}}  \tag{6}
$$

其中$S={s_{ij}|1\leq i,j\geq N}$和$\widetilde{S}={\widetilde{s}_{ij}|1\leq i,j\geq N}$分别是初始和观测到的边权重值。有效值$\widetilde{s}_{ij}$可以通过下式得出：

$$
\tilde{s}_{i j}=\tilde{\mathbf{f}}_{i}^{\top} \tilde{\mathbf{f}}_{j}=\bar{f}\left(u_{i} ; \theta^{t+1}\right)^{\top} \bar{f}\left(u_{j} ; \theta^{t+1}\right) \tag{7}
$$

其中$u_i$和$u_j$分别是分配给$\overline{v}_i$和$\overline{v}_j$的提前保存的图片。由于$\overline{v}_i$定义了特征空间中第i个区域，拓扑保持损失项隐式地维持了这些区域的连通性。这个损失项的另一个选择是惩罚相似度的$l_1$或者$l_2$范数。在作者的实验中，他们发现这种限制形成关系灵活而且会发生错误，因为他们不允许相似度的线性变化。

拓扑损失惩罚节点之间拓扑结构关系的改变的同时允许节点合理的变化，而不是惩罚特征空间中弹性赫布图的节点的变化。这种限制是软性的，更容易优化，这使得弹性赫布图的弹性不会影响到新类。

## 优化

拓扑类增量学习将CNN模型和弹性赫布图$G^t$相结合，其中$G^t$被用于保存CNN特征空间流型的拓扑结构。值得注意的是CNN模型使用小批量随机梯度下降进行训练，而$G^t$通过竞争性赫布学习构建和更新。在每个小批量的迭代中用公式（4）更新$G^t$中的节点是低效的，因为中间过程中获取到的特征还没有被完全优化。因此，作者在CNN完全训练完毕后才进行$G^t$的学习。然后$G^t$会用于下一个增量组(t+1)

## 和基于蒸馏方法的对比

和本文的维持特征空间拓扑的方法相反，其他类增量学习的工作大多基于知识蒸馏，其中蒸馏损失项会加到交叉熵损失上：

$$
\ell\left(\tilde{X}^{t+1} ; \theta^{t+1}, \theta^{t}\right)=\ell_{C E}\left(\tilde{X}^{t+1} ; \theta^{t+1}\right)+\gamma \ell_{D L}\left(\tilde{X}^{t+1} ; \theta^{t+1}, \theta^{t}\right)
$$

其中$\widetilde{X}^{t+1}=X^{t+1}\cup M^t$表示新类训练样本$X^{t+1}$和旧类样本的并集，$\theta^t$和$\theta^{t+1}$是在t组和t+1组获得到的参数集。蒸馏损失项根据旧类被应用到网络的输出logits：

$$
\ell_{D L}\left(\tilde{X}^{t+1} ; \theta^{t+1}, \theta^{t}\right)=-\sum_{(x, y) \in \tilde{X}^{t+1}} \sum_{c=1}^{C^{t}} \frac{e^{-o_{c}\left(x ; \theta^{t}\right) / T}}{\sum_{j=1}^{C^{t}} e^{-o_{j}\left(x ; \theta^{t}\right) / T}} \log \frac{e^{-o_{c}\left(x ; \theta^{t+1}\right) / T}}{\sum_{j=1}^{C^{t}} e^{-o_{j}\left(x ; \theta^{t+1}\right) / T}},
$$

其中$C^t$是旧类的数量，T是蒸馏温度。另一个蒸馏方法是将蒸馏损失应用到特征空间上，这被称作特征蒸馏损失：

$$
\ell_{F D L}\left(\tilde{X}^{t+1} ; \theta^{t+1}, \theta^{t}\right)=\sum_{(x, y) \in \tilde{X}^{t+1}}\left(1-\bar{f}\left(x ; \theta^{t}\right)^{\top} \bar{f}\left(x ; \theta^{t+1}\right)\right),
$$

其中$\overline{f}(x;\theta^t)$表示归一化的特征向量。

这样的限制太严格了并且难以获得满意的效果，因为是交叉熵损失在特征空间中适应新类起主要作用。作者已经观察到了从头开始现象，即旧类样本一开始的时候还是被忘记了，只是之后用多个周期给他找补回来了。相比之下，拓扑保持损失项约束弹性赫布图节点的邻接关系，允许特征空间更自由地适应新类而不会丢失辨别力。